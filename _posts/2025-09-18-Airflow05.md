---
title: "Airflow DAG 파라미터 & 오퍼레이터 정리"
author: mminzy22
date: 2025-09-19 02:00:00 +0900
categories: [Airflow, Amazon MWAA]
tags: [TIL, Airflow, Amazon MWAA, DAG, Operator]
description: "Apache Airflow DAG 작성 시 자주 사용하는 파라미터 옵션들과 대표 오퍼레이터들을 공식문서를 기반으로 정리"
pin: false
mermaid: true
---


Apache Airflow에서 DAG(Directed Acyclic Graph)를 정의할 때는 단순히 태스크의 순서를 지정하는 것뿐 아니라 다양한 실행 옵션을 설정할 수 있다. 또한 **Operator**를 통해 실제 작업 단위를 정의하며, 데이터 이동·API 호출·조건 분기·알림 등 다양한 목적에 맞는 오퍼레이터들이 존재한다.

---

## DAG 기본 구조 예시

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.bash import BashOperator

with DAG(
    dag_id="tutorial",
    description="A simple tutorial DAG",
    schedule_interval=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    default_args={
        "depends_on_past": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    tags=["example"],
) as dag:

    t1 = BashOperator(
        task_id="print_date",
        bash_command="date",
    )

···

```

---

## 주요 파라미터 정리

| 파라미터                      | 설명                                          | 기본값                    | 예시                                | 활용 사례             |
| ------------------------- | ------------------------------------------- | ---------------------- | --------------------------------- | ----------------- |
| **dag_id**               | DAG 고유 식별자                                  | 필수                     | `dag_id="tutorial"`               | DAG 관리, 모니터링      |
| **description**           | DAG 설명문                                     | 없음                     | `description="ETL Pipeline"`      | 문서화, UI 가독성       |
| **schedule_interval**    | DAG 실행 주기                                   | 없음                     | `schedule_interval="0 12 * * *"`  | 매일 정오 실행          |
| **start_date**           | DAG 실행 시작일                                  | 없음                     | `datetime(2021,1,1)`              | 실행 시작 시점 설정       |
| **catchup**               | 과거 기간에 대한 DAG 실행 여부                         | `True`                 | `False`                           | 백필 여부 제어          |
| **tags**                  | DAG에 태그 지정                                  | 없음                     | `tags=["etl", "example"]`         | UI 검색/필터링         |
| **default_args**         | DAG 내 모든 태스크에 공통 적용되는 기본 인자                 | `{}`                   | `{ 'retries': 1 }`                | 공통 정책 적용          |
| **depends_on_past**     | 이전 실행(Task Instance)이 성공해야 현재 실행 진행 여부 결정   | `False`                | `True`                            | 순차적 종속성 있는 ETL    |
| **retries**               | 실패 시 재시도 횟수                                 | `0`                    | `retries=3`                       | 일시적 오류 대응         |
| **retry_delay**          | 재시도 간격                                      | `timedelta(minutes=5)` | `timedelta(minutes=10)`           | API rate limit 대응 |
| **max_retry_delay**     | 재시도 간격의 최대 한도                               | 없음                     | `timedelta(hours=1)`              | 지수적 백오프 제한        |
| **queue**                 | 실행할 워커 큐 지정                                 | `default`              | `queue='bash_queue'`              | 워커 자원 분리          |
| **pool**                  | 동시 실행 개수를 제한하는 풀 지정                         | 없음                     | `pool='db_pool'`                  | DB 연결 보호          |
| **priority_weight**      | 스케줄링 우선순위 가중치                               | `1`                    | `priority_weight=10`              | 중요 태스크 우선 실행      |
| **end_date**             | DAG 실행 종료일                                  | 없음                     | `datetime(2025, 12, 31)`          | 특정 기간 한정 캠페인      |
| **wait_for_downstream** | 이전 DAG 실행의 **다운스트림(Task 이후 단계)** 완료까지 대기 여부 | `False`                | `True`                            | 데이터 일관성 보장        |
| **execution_timeout**    | 태스크 최대 실행 시간                                | 없음                     | `timedelta(minutes=30)`           | 무한 대기 방지          |
| **sla**                   | 태스크 완료 목표 시간                                | 없음                     | `sla=timedelta(hours=1)`          | SLA 모니터링          |
| **trigger_rule**         | 태스크 실행 조건                                   | `all_success`          | `trigger_rule='all_failed'`       | 특정 조건에서 실행        |
| **on_failure_callback** | 실패 시 실행할 함수                                 | 없음                     | `on_failure_callback=alert_func`  | Slack/Email 알림    |
| **on_success_callback** | 성공 시 실행할 함수                                 | 없음                     | `on_success_callback=log_func`    | 성공 로깅             |
| **on_retry_callback**   | 재시도 시 실행할 함수                                | 없음                     | `on_retry_callback=retry_alert`   | 재시도 모니터링          |
| **sla_miss_callback**   | SLA 위반 시 실행할 함수                             | 없음                     | `sla_miss_callback=notify_func`   | SLA 경고            |
| **on_skipped_callback** | 태스크 건너뜀 시 실행할 함수                            | 없음                     | `on_skipped_callback=skip_logger` | 조건부 태스크 추적        |
| **email**                 | 알림 메일 주소 (리스트)                              | 없음                     | `email=['dev@team.com']`          | 실패 시 이메일 알림       |
| **email_on_failure**    | 실패 시 이메일 전송 여부                              | `False`                | `True`                            | 자동 실패 알림          |
| **email_on_retry**      | 재시도 시 이메일 전송 여부                             | `False`                | `True`                            | 재시도 상황 모니터링       |
| **resources**             | 태스크 실행 시 필요한 리소스(CPU, 메모리 등) 제약 설정          | 없음                     | `resources={'cpus':1,'ram':512}`  | 리소스 관리            |
| **doc_md / doc**         | 태스크/DAG에 대한 문서화 (Markdown, plain text 등)    | 없음                     | `t1.doc_md = "### 설명..."`         | UI에 문서 표시         |
| **task_concurrency**     | 태스크 인스턴스의 동시 실행 개수 제한                       | 없음                     | `task_concurrency=1`              | 중복 실행 방지          |

---

## 실행 조건 (Trigger Rules)

Airflow는 DAG 실행 시 각 태스크가 언제 실행될지를 **Trigger Rule**로 제어한다.

| Trigger Rule  | 설명                         |
| ------------- | -------------------------- |
| `all_success` | 모든 업스트림 태스크가 성공해야 실행 (기본값) |
| `all_failed`  | 모든 업스트림 태스크가 실패해야 실행       |
| `one_success` | 업스트림 중 하나라도 성공 시 실행        |
| `one_failed`  | 업스트림 중 하나라도 실패 시 실행        |
| `none_failed` | 업스트림이 실패하지 않았을 때 실행        |
| `always`      | 업스트림 결과와 관계없이 항상 실행        |

---

## 주요 오퍼레이터 정리

Airflow의 **Operator**는 태스크에서 실제로 수행할 작업을 정의한다. 목적별로 다양한 오퍼레이터가 제공된다.

| Operator                                 | 설명                       | 활용 예시                  |
| ---------------------------------------- | ------------------------ | ---------------------- |
| **BashOperator**                         | Bash 명령어 실행              | `bash_command="date"`  |
| **PythonOperator**                       | Python 함수 실행             | 데이터 전처리 함수 실행          |
| **BranchPythonOperator**                 | 조건 분기 실행                 | 특정 조건에 따라 다른 태스크 실행    |
| **DummyOperator / EmptyOperator**        | 실제 작업 없이 DAG 구조만 유지      | 테스트, 태스크 연결용           |
| **EmailOperator**                        | 이메일 발송                   | 실패/성공 알림 메일 전송         |
| **SimpleHttpOperator**                   | HTTP API 호출              | REST API 호출            |
| **Sensor (예: S3KeySensor)**              | 특정 조건 만족 시까지 대기          | S3 객체 존재 확인 후 다음 단계 진행 |
| **S3ToRedshiftOperator**                 | AWS S3 → Redshift 데이터 적재 | 데이터 웨어하우스 로드           |
| **GoogleCloudStorageToBigQueryOperator** | GCS → BigQuery 데이터 적재    | GCP 데이터 파이프라인          |
| **DockerOperator**                       | Docker 컨테이너 실행           | 특정 환경 격리 실행            |
| **KubernetesPodOperator**                | Kubernetes Pod 실행        | 클러스터 기반 확장 실행          |

---

## 정리

* DAG 파라미터는 단순 옵션이 아니라 **워크플로우의 신뢰성과 운영 효율성**을 좌우한다.
* `retries`, `timeout`, `queue`, `pool` 등은 장애 대응과 리소스 관리에 핵심적이다.
* Operator는 목적에 맞게 선택해야 하며, ETL/데이터 이동/알림/조건 분기/컨테이너 실행 등 다양한 상황에서 조합 가능하다.
* 팀 단위 운영에서는 `callback`, `SLA`, `resources` 옵션과 함께 **Transfer·Sensor 오퍼레이터**를 적절히 활용하는 것이 권장된다.
